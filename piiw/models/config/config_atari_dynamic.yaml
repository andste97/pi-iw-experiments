project_name: "pi-iw-experiments-piiw"
model:
  conv1_in_channels: 4
  conv1_out_channels: 16 # also conv1 out_channels
  conv1_kernel_size: 8
  conv1_stride: 4

  conv2_out_channels: 32
  conv2_kernel_size: 4
  conv2_stride: 2

  fc1_in_features: 2592 # = 32*9*9
  fc1_out_features: 256 # also output layer in_features
  # num_logits: 5 # This value needs to be set dynamically as it depends on the
                  # size of the environment action space. For gridenvs: 5
  add_value: False # Instead of a policy this will output the action value
  use_dynamic_features: True # set this to true to add dynamic features to model output

plan:
  interactions_budget: 100 # This had been increased to 100 according to the paper
  width: 1  # if we use width 1 here, the planner fails. This is a width 2 problem so this is expected.
  discount_factor: 0.99
  cache_subtree: True
  softmax_temperature: 0.5

train:
  env_id: "Pong-v4"
  optim: "rmsprop"
  seed: 0
  batch_size: 32
  episode_length: 18000 # They used 18.000 steps according for this according to phd thesis
  learning_rate: 0.0005
  replay_capacity: 10000
  total_interaction_budget: 20000000
  l2_reg_factor: 0.001
  clip_grad_norm: 40
  rmsprop_alpha: 0.99 # same as rho in tf
  rmsprop_epsilon: 0.1
  add_returns: False
  experience_keys: ["observations", "target_policy"] # can also add "returns" here
  atari_frameskip: 15
  # max_epochs: 15 # number of epochs the trainer will train the model for
  max_steps: 270000 # max number of steps the trainer will train the model for
                    # using steps instead of epochs ensures every model gets the same number of training steps